{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ1HjQc8vvtG",
        "outputId": "82531096-330f-4d4f-9ea8-5789faa3a540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "      ALLERGY       1.00      0.25      0.40         4\n",
            "   ASSESSMENT       0.00      0.00      0.00         4\n",
            "           CC       0.25      0.25      0.25         4\n",
            "    DIAGNOSIS       1.00      1.00      1.00         1\n",
            "  DISPOSITION       0.50      1.00      0.67         2\n",
            "     EDCOURSE       0.00      0.00      0.00         3\n",
            "         EXAM       0.33      1.00      0.50         1\n",
            "    FAM/SOCHX       0.92      1.00      0.96        22\n",
            "        GENHX       0.59      0.95      0.73        20\n",
            "        GYNHX       0.00      0.00      0.00         1\n",
            "      IMAGING       0.00      0.00      0.00         1\n",
            "IMMUNIZATIONS       1.00      1.00      1.00         1\n",
            "         LABS       0.00      0.00      0.00         1\n",
            "  MEDICATIONS       0.57      0.57      0.57         7\n",
            "OTHER_HISTORY       0.00      0.00      0.00         1\n",
            "PASTMEDICALHX       0.80      1.00      0.89         4\n",
            " PASTSURGICAL       0.80      1.00      0.89         8\n",
            "         PLAN       0.00      0.00      0.00         3\n",
            "   PROCEDURES       0.00      0.00      0.00         1\n",
            "          ROS       0.83      0.45      0.59        11\n",
            "\n",
            "     accuracy                           0.69       100\n",
            "    macro avg       0.43      0.47      0.42       100\n",
            " weighted avg       0.63      0.69      0.63       100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# read data\n",
        "train_data = pd.read_csv('TaskA-TrainingSet.csv')\n",
        "valid_data = pd.read_csv('TaskA-ValidationSet.csv')\n",
        "test_data = pd.read_csv('taskA_testset4participants_headers_inputConversations.csv')\n",
        "\n",
        "# assign data and labels\n",
        "X_train = train_data['dialogue']\n",
        "y_train = train_data['section_header']\n",
        "X_valid = valid_data['dialogue']\n",
        "y_valid = valid_data['section_header']\n",
        "X_test = test_data['dialogue']\n",
        "\n",
        "# data cleaning/preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "def clean_data(text):\n",
        "    \n",
        "    # remove \"Doctor:\" and \"Patient:\" labels and timestamps\n",
        "    text = re.sub(r\"(Doctor|Patient|Guest_family):|\\d{1,2}[:.]\\d{1,2}\\s?(AM|PM|am|pm)?\", \"\", text)\n",
        "    \n",
        "    # lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove special characters and digits\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]+\", \"\", text)\n",
        "\n",
        "    # tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # join words back into a single string\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "X_train = X_train.apply(clean_data)\n",
        "X_valid = X_valid.apply(clean_data)\n",
        "X_test = X_test.apply(clean_data)\n",
        "\n",
        "## tfidf_vec = TfidfVectorizer(stop_words='english')\n",
        "tfidf_vec = TfidfVectorizer(stop_words='english',\n",
        "                            ngram_range=(1, 2),\n",
        "                            max_df=0.9,\n",
        "                            # min_df=2,\n",
        "                            #max_features=300\n",
        "                           )\n",
        "X_train = tfidf_vec.fit_transform(X_train)\n",
        "X_valid = tfidf_vec.transform(X_valid)\n",
        "X_test = tfidf_vec.transform(X_test)\n",
        "\n",
        "## oversampling\n",
        "ros = RandomOverSampler(random_state = 712)\n",
        "X_train, y_train = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# LogisticRegression model\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lr = LogisticRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "# label prediction of validation data\n",
        "y_pred = model_lr.predict(X_valid)\n",
        "report = classification_report(y_valid, y_pred)\n",
        "print(report)\n",
        "\n",
        "y_test = model_lr.predict(X_test)\n",
        "test_data.rename({'ID': 'TestID'}, axis = 1, inplace = True)\n",
        "test_data.drop('dialogue', axis = 1, inplace = True)\n",
        "test_data.set_index('TestID', inplace = True)\n",
        "test_data['SystemOutput'] = y_test\n",
        "\n",
        "test_data.to_csv('taskA_StellEllaStars_run3_mediqaSum.csv')\n",
        "\n",
        "## LogisticRegression GridSearchCV\n",
        "#param_grid = {\n",
        "#    'C': [0.1, 1, 10],\n",
        "#    'penalty': ['l1', 'l2'],\n",
        "#    'solver': ['liblinear', 'saga'],\n",
        "#    'max_iter': [100, 200, 500]\n",
        "#}\n",
        "#model_lr = LogisticRegression()\n",
        "#grid_search = GridSearchCV(model_lr, param_grid, cv = 5)\n",
        "#grid_search.fit(X_train, y_train)\n",
        "#print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "#print(\"Best score: \", grid_search.best_score_)\n",
        "#best_model = grid_search.best_estimator_\n",
        "#y_pred = best_model.predict(X_valid)\n",
        "#report = classification_report(y_valid, y_pred)\n",
        "#print(report)\n",
        "\n"
      ]
    }
  ]
}